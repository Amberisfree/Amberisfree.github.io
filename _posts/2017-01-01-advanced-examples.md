---
title:  "Performance Modelling"
mathjax: true
layout: post
categories: media
---



##  Our Estimated  Data Access

| Header 1 | Number of Data Access | Number of Floating Point throughput | Best Case Operational Intensity (Flops/ Bytes) |
|----------|----------|----------|----------|
| function_a | $N^{2} +4N$ | $2N^{2}$ |$\frac{2N^{2}}{8({N^2}+4N)} =\frac{N^{2}}{4({N^2}+4N)}  \approx 1/4$ |
| function_b | $3N+1$      | $2N$     |$\frac{2N}{8(3N+1)}=\frac{N}{4(3N+1)} \approx 1/12$ |
| function_c | $3N+1$      | $1.5N$     |$\frac{3/2N}{8(3N+1)}=\frac{1}{16(N+1)} \approx 1/16$ |
| function_d | $3N+1$      | $2N$     |$\frac{2N}{8(3N)}=1/12$ |

#### Data Access and FLoating Point Throughput is calculated by hand
#### Operational Intensity Explained
Operational Intensity, which measures traffic between the caches and memory rather than between the processors and the caches, is calculated by flops/bytes; in order to calculate bytes transferred, we have to multiply number of data access by 8, because the data type is double with 8 bytes.

##  Total Execution Time 

We use  `the profiling tool: gprof`to determine the overall execution time and the hotspot function(s) that requires the most execution time. In order to determine the influence of the problem size, we carry out the profiling for N=k·104, where k=1,2,3,4,5,6,7,8,9,10. See the table:


{% highlight c %}

static void asyncEnabled(Dict* args, void* vAdmin, String* txid, struct Allocator* requestAlloc)
{
    struct Admin* admin = Identity_check((struct Admin*) vAdmin);
    int64_t enabled = admin->asyncEnabled;
    Dict d = Dict_CONST(String_CONST("asyncEnabled"), Int_OBJ(enabled), NULL);
    Admin_sendMessage(&d, txid, admin);
}

{% endhighlight %}

### Call Graph Summary Generated by grof

| Time | Self | Children | Name |
|----------|----------|----------|----------|
| 100 | 0.00 | 0.61 | main |
|  | 0.35| 0.00 | function_a |
|  | 0.26 | 0.00 | init_datastructures |
|  | 0.00 | 0.00 | function_b |
|  | 0.00 | 0.00 | function_c |
|  | 0.00 | 0.00 | function_d |

We used 10000 as our problem size to profile with gprof. We observed that the hotspot function generally lies in function_a, with 0.35 total time spent on itself, whereas function_b, function_c, and function_d barely take any time to be executed. This is reasonable because function_a’s number of data access and number of floating point throughput is far bigger than the other three functions. By focusing optimization efforts on function_a, we can potentially achieve significant improvements in the overall performance of your application.


## Performance of Hotspot Function

We used `likwid tool`  to profile the memory and floating point performance of the hotspot function as determined in Task (1.2) for N = 104. According to the result metric, the observed Operational intensity [FLOP/Byte] for our hotspot function (function_a) is  0.2417, which corresponds to our calculation in the task 1.1. 


<script src="https://gist.github.com/5555251.js?file=gist.md"></script>

## Roofline Analysis

We visualized the results obtained for the hotspot function as determined by call graph in the form of a roofline model. Then, we outlined some techniques that could be applied to optimize the execution of time on CPUs. Our roofline model is Hamilton, which uses AMD EPTC 7702 64-Core Processor ( peak floating point performance is 53.5 GFlops/s and peak streaming memory bandwidths is 24000).

According to `our roofline model`, we can either maximize data bandwidth or minimizing traffic: 

    Firstly, our resource bottleneck is primarily limited by data transfer, because these points lie to the left of the ridge points. We suggest that our memory bottlenecks can be reduced with restructuring loops for unit stride access, ensure memory affinity, and use software prefetching. 
	Secondly, on efficiency level, the traffic between the caches and DRAM is measured as the operational intensity, which increases with problem size and is approximately around 1/4 that matches our calculated best case operational intensity for function_a.  
	Thirdly, since memory performance is defined by the memory system behind the caches
Perfect cache VS. pessimal cache……………………………




![Flower](https://user-images.githubusercontent.com/4943215/55412447-bcdb6c80-5567-11e9-8d12-b1e35fd5e50c.jpg)

[Flower](https://unsplash.com/photos/iGrsa9rL11o) by Tj Holowaychuk

## Optimization: see the next page

We will proceed onto our next level of GPU programming to provide a overall framework in our code optimization techniques 

{% include embed.html url="https://www.youtube.com/embed/_C0A5zX-iqM" %}
